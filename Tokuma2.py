import random
import time
from selenium import webdriver
from selenium.webdriver.chrome import service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

import requests
from bs4 import BeautifulSoup

import re

import pandas as pd

from selenium.webdriver.common.action_chains import ActionChains

from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome import service as fs

from os.path import join #æ—¢ã«é–‹ã„ã¦ã„ã‚‹ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å–å¾—ã™ã‚‹éš›ã®ã‚³ãƒ¼ãƒ‰ã«å¿…è¦

import chromedriver_binary   #Chromeã®ãƒ‘ã‚¹ã‚’é€šã™ãŸã‚ã®ã‚³ãƒ¼ãƒ‰
#èµ·å‹•ã—ãŸã‚‰ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä½¿ç”¨ã€ãã“ã§é–‹ã‹ã‚ŒãŸãƒ–ãƒ©ã‚¦ã‚¶ã§privacypassã‚’èµ·å‹•ã—èªè¨¼ã‚’è¡Œã†
#/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir="/Users/satousuguru/Desktop/Python/google chrome" âš ï¸æœ€å¾Œã®ã‚¯ã‚ªãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ã‚“å…¥ã‚Œãªã„ã¨ãƒã‚°ã‚‹
#https://qiita.com/mimuro_syunya/items/2464cd2404b67ea5da56:å‚è€ƒ
#:ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä½¿ç”¨

EXTENSION_PATH = "/Users/satousuguru/Desktop/Python/KeywordSurfer.crx" #âš ï¸æ‹¡å¼µæ©Ÿèƒ½ã®ãƒ‘ã‚¹ã€‚é©å®œæ›¸ãæ›ãˆã¦

import csv #csvã‚’èª­ã¿è¾¼ã‚€

#csvã®èª­ã¿è¾¼ã¿åŠã³ä¼šç¤¾åã ã‘ã«çµã‚‹
df = pd.read_csv('ãƒãƒ­ãƒ¯_ãƒˆã‚™ãƒ©ã‚¤ãƒã‚™ãƒ¼_æ±äº¬ - ãƒˆã‚¯ãƒæ¡ˆä»¶.csv')
df_company_name_only = df['ä¼šç¤¾å']

#foræ–‡ã®ä¸‹æº–å‚™
need_deleted = "æ”¯åº—"or"äº‹æ¥­éƒ¨"or"å–¶æ¥­æ‰€"or"äº‹æ¥­æ‰€"
company_name_list=[]

#dfã‹ã‚‰listã«å¤‰æ›ã¨ä¸è¦éƒ¨åˆ†ã®å‰Šé™¤ä½œæ¥­
for company_name in df_company_name_only:
    for row in df_company_name_only:

        #æ”¯åº—ãªã©ãŒã‚ã‚‹å ´åˆã€ãã®éƒ¨åˆ†ã‚’å‰Šé™¤
        if "æ”¯åº—" in row or "äº‹æ¥­éƒ¨" in row or "å–¶æ¥­æ‰€" in row or "å–¶æ¥­æ‰€" in row or "äº‹æ¥­æ‰€" in row or "æ”¯ç¤¾" in row :
        #âš ï¸if æ–‡å­—åˆ—inãªã‚“ã¨ã‹ã®æ™‚ã¯ã€æ–‡å­—åˆ—oræ–‡å­—åˆ—or.. in rowã§ã¯ãªãã€ã“ã®ã‚ˆã†ãªå½¢ã§æ›¸ã‹ãªã„ã¨ãƒã‚°ã‚‹
            print(f"å‰Šé™¤ã™ã‚‹æ–‡å­—åˆ—ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€ã€Œ{row}ã€ã‹ã‚‰")
            row_need_delete = row.split("\u3000")[-1]
            row = row.replace(row_need_delete,"")
            print(f"ã€Œ{row}ã€ã«å¤‰æ›´ã—ã¾ã—ãŸ")
        company_name_list.append(row)

#setå‹ã‚’ç”¨ã„ã¦ã€é‡è¤‡ã™ã‚‹ä¼šç¤¾åã‚’å‰Šé™¤
company_name_list = list(set(company_name_list))
print(company_name_list)


##ã“ã“ã‹ã‚‰ä¸Šã®ãƒªã‚¹ãƒˆã‚’ç”¨ã„ã¦å®Ÿéš›ã«æ¤œç´¢ã—ã¦ã„ã

#ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼ã®æº–å‚™
CHROMEDRIVER = "/Users/satousuguru/Desktop/chromedriver"

root = join(__file__, "..")

# webdriverã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œã‚‹ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãŒé–‹ãï¼‰
browser_path=join(root, "/Users/satousuguru/Desktop/chromedriver")

# èµ·å‹•æ™‚ã«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ã¤ã‘ã‚‹ã€‚ï¼ˆãƒãƒ¼ãƒˆæŒ‡å®šã«ã‚ˆã‚Šã€èµ·å‹•æ¸ˆã¿ã®ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’å–å¾—ï¼‰
options = webdriver.ChromeOptions()
options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
browser = webdriver.Chrome(executable_path=browser_path, options=options)

#æ‹¡å¼µæ©Ÿèƒ½ã‚’è¨­å®š
chrome_service = fs.Service(executable_path=CHROMEDRIVER)
"""
options = webdriver.ChromeOptions()
"""
options.add_argument(f'service={chrome_service}')
options.add_extension(EXTENSION_PATH)
browser = webdriver.Chrome(options=options)




#æ‹¡å¼µæ©Ÿèƒ½ã§passã‚’ç²å¾—ã—ã¦ã„ã‚‹é–“ã€ä¸­æ–­
k = 0
while k == 0:
    start = input(print("ãƒ–ãƒ©ã‚¦ã‚¶ã®æ‹¡å¼µæ©Ÿèƒ½(privacy pass)ã‚’é¸æŠã—ã€get more passã‚’è¡Œã£ã¦ãã ã•ã„ã€‚æ±‚äººèª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã™ã‚‹å ´åˆã¯cã¨æ›¸ã„ã¦ãã ã•ã„"))
    if start == "c":
        break
    else:
        continue

#indeedã®ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã€ã¾ãŸãã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®å–å¾—
browser.get("https://jp.indeed.com/?from=gnav-viewjob")
source = browser.page_source
soup = BeautifulSoup(source,"html.parser")

actions = ActionChains(browser)


ini_or_not = 0 #åˆå›ã‹ã©ã†ã‹åˆ¤å®š
count = 0 #
error_count = 0
url_count = 0 #URlå†…ã®start=ä»¥é™ã®ã‚«ã‚¦ãƒ³ãƒˆ
page_num=0 #ãƒšãƒ¼ã‚¸ãŒè¤‡æ•°ã«æ¸¡ã‚‹å ´åˆã®ç•ªå·

for search_company_name in company_name_list:
    #åˆå›ã®æ¤œç´¢ã¨ãã‚Œä»¥é™ã¯ã€æœ€åˆã®ãƒšãƒ¼ã‚¸ãŒé•ã†ã®ã§ãƒã‚°ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã‚‚ä¸€å¿œä¾¿å®œä¸Šåˆ†ã‘ã¦ã‚‹ã€‚ãŸã çµæœçš„ã«è¦‹ãŸã‚‰ãã‚“ãªæ„å‘³ãªã„
    if ini_or_not == 0:

        print(f"åˆå›ã®æ¤œç´¢ã§ã™ã€‚ç¾åœ¨ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯{search_company_name}ã§ã™")

        word_number = len(search_company_name)
        print(word_number)

        elem_type_search_word = browser.find_element(By.ID,"text-input-what")
        elem_type_search_word.send_keys(f'"{search_company_name}"')

        elem_search_btn = browser.find_element(By.CLASS_NAME,"yosegi-InlineWhatWhere-primaryButton")
        elem_search_btn.click()

        source = browser.page_source
        soup = BeautifulSoup(source,"html.parser")

        job_list = soup.find("ul",attrs={"class":"jobsearch-ResultsList css-0"})
        if job_list is None: #æ±‚äººæ•°ãŒ0ã®æ™‚ã¯ã“ã“ã§ãƒ«ãƒ¼ãƒ—æœ€åˆã«æˆ»ã‚‹
            print("ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸğŸ˜¢")
            time.sleep(3)
            continue

        job_list = job_list.find_all("div",attrs={"class":"slider_container css-g7s71f eu4oa1w0"})#æ±‚äººã‚’ãã‚Œãã‚Œèª­ã¿è¾¼ã‚€
        print(f"ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººæ•°ã¯{len(job_list)}")

        pay = 0
        more = 0
        for job in job_list:
            if job.find("span",attrs={"class":"iaIcon"}) and job.find("span",attrs={"class":"more_loc_container"}):#å¿œå‹ŸãŒã‚ã‚Šã€ã‹ã¤ãã®ä»–ã®å‹¤å‹™åœ°ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
                other_area = job.find("span",attrs={"class":"more_loc_container"}).text #ãã®ä»–ã®å‹¤å‹™åœ°(3)
                other_area_num = re.findall(r"\d+", other_area) #æ•°å­—éƒ¨åˆ†ã®ã¿ã‚’å–ã‚Šå‡ºã™
                other_area_num = other_area_num[0]
                other_area_num = int(other_area_num)
                more += other_area_num

            if job.find("span",attrs={"class":"iaIcon"}):#indeedã§å¿œå‹Ÿã€ã‚‚ã—ãã¯ã‚«ãƒ³ã‚¿ãƒ³å¿œå‹Ÿã‚’æ¤œå‡º(ãã®å·¦ã«ã‚ã‚‹çŸ¢å°ã‚¢ã‚¤ã‚³ãƒ³ã‚’åˆ¤å®šã«ä½¿ã£ã¦ã„ã‚‹)
                pay +=1

            else:
                continue

        print(f"æœ‰æ–™ã®æ•°ã¯{pay}å€‹")
        print(f"ãã®ä»–ã®å‹¤å‹™åœ°ã‚‚å«ã‚ã‚‹ã¨{pay + more}å€‹ã«ãªã‚Šã¾ã™")


        ini_or_not+=1

    else:

        ##ã¾ã å‰ã®ãƒšãƒ¼ã‚¸##
        sleep_time = random.uniform(10,12)
        time.sleep(sleep_time)



        elem_type_search_word = browser.find_element(By.ID,"text-input-what")
        k = 0
        while k < word_number*2: #ç‰¹ã«äºŒå€ã®æ„å‘³ã¯ãªã„ã‘ã©ãªã‚“ã¨ãªãæ€–ã„ã‹ã‚‰ã€‚ãƒã‚°ãŒå‡ºã‚‹ãªã‚‰ã“ã“ã®å¯èƒ½æ€§ã‚ã‚Š
            elem_type_search_word.send_keys(Keys.BACK_SPACE)
            k+=1
        elem_type_search_word.send_keys(f'"{search_company_name}"')

        print(f"ç¾åœ¨ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯{search_company_name}ã§ã™")
        word_number = len(search_company_name)


        elem_search_btn = browser.find_element(By.CLASS_NAME,"yosegi-InlineWhatWhere-primaryButton")
        elem_search_btn.click()

        ##ã“ã“ã‹ã‚‰æ¬¡ã®ãƒšãƒ¼ã‚¸##

        source = browser.page_source
        soup = BeautifulSoup(source,"html.parser")

        job_list = soup.find("ul",attrs={"class":"jobsearch-ResultsList css-0"})
        if job_list is None: #æ±‚äººæ•°ãŒ0ã®æ™‚ã¯ã“ã“ã§ãƒ«ãƒ¼ãƒ—æœ€åˆã«æˆ»ã‚‹
            print("ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸğŸ˜¢")
            continue

        if len(job_list)>=15:
            print("ãƒšãƒ¼ã‚¸ãŒè¤‡æ•°ã«æ¸¡ã‚Šã¾ã™ã€‚ç›´ä¸‹ã«è¨˜è¼‰ã™ã‚‹æƒ…å ±ã¯1ãƒšãƒ¼ã‚¸ç›®ã®ã¿ã«ã¤ã„ã¦ã®æƒ…å ±ã§ã™ã€‚")

        job_list = job_list.find_all("div",attrs={"class":"slider_container css-g7s71f eu4oa1w0"})#æ±‚äººã‚’ãã‚Œãã‚Œèª­ã¿è¾¼ã‚€
        print(f"ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººæ•°ã¯{len(job_list)}")

        pay = 0
        more = 0
        for job in job_list:
            if job.find("span",attrs={"class":"iaIcon"}) and job.find("span",attrs={"class":"more_loc_container"}):#å¿œå‹ŸãŒã‚ã‚Šã€ã‹ã¤ãã®ä»–ã®å‹¤å‹™åœ°ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
                other_area = job.find("span",attrs={"class":"more_loc_container"}).text #ãã®ä»–ã®å‹¤å‹™åœ°(3)
                other_area_num = re.findall(r"\d+", other_area) #æ•°å­—éƒ¨åˆ†ã®ã¿ã‚’å–ã‚Šå‡ºã™
                other_area_num = other_area_num[0]
                other_area_num = int(other_area_num)

                more += other_area_num

            if job.find("span",attrs={"class":"iaIcon"}):#indeedã§å¿œå‹Ÿã€ã‚‚ã—ãã¯ã‚«ãƒ³ã‚¿ãƒ³å¿œå‹Ÿã‚’æ¤œå‡º(ãã®å·¦ã«ã‚ã‚‹çŸ¢å°ã‚¢ã‚¤ã‚³ãƒ³ã‚’åˆ¤å®šã«ä½¿ã£ã¦ã„ã‚‹)
                pay +=1

            else:
                continue

        print(f"æœ‰æ–™ã®æ•°ã¯{pay}å€‹")
        print(f"ãã®ä»–ã®å‹¤å‹™åœ°ã‚‚å«ã‚ã‚‹ã¨{pay + more}å€‹ã«ãªã‚Šã¾ã™")

        count += 1

        if len(job_list) is None:
            break

        additional_pay_none = 0
        while len(job_list) == 15: #ãƒšãƒ¼ã‚¸ãŒã¾ã ç¶šãå ´åˆã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã«ç§»å‹•
            if url_count == 0: #æœ€åˆã®å ´åˆ(urlã®æ§‹é€ ãŒé•ã†)
                print("æ±‚äººæ•°ãŒè¦å®šæ•°ä»¥ä¸Šã®ãŸã‚ã€ãƒšãƒ¼ã‚¸ã‚’é·ç§»ã—ã¾ã™")
                url_count+=10
                cur_url = browser.current_url
                cur_url = cur_url.split("&")[0]
                cur_url_next = cur_url +"&&start=" + str(url_count)
                print(cur_url)
                print(cur_url_next)
                browser.get(f"{cur_url_next}")
                page_num +=1

                sleep_time = random.uniform(8,12) #é·ç§»å‰ã«æ™‚é–“ã‚’ç½®ã
                time.sleep(sleep_time)

                ##ã“ã“ã‹ã‚‰æ¬¡ã®ãƒšãƒ¼ã‚¸

                source = browser.page_source
                soup = BeautifulSoup(source,"html.parser")

                job_list = soup.find("ul",attrs={"class":"jobsearch-ResultsList css-0"})
                if job_list is None: #2ãƒšãƒ¼ã‚¸ç›®ã®æ±‚äººæ•°ãŒ0ã®æ™‚ã¯ã“ã“ã§ä½•ã‚‚ã›ãšãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚‹
                    print("2ãƒšãƒ¼ã‚¸ç›®ã«ã¯æ±‚äººãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸğŸ˜¢ç›´ä¸Šã®æƒ…å ±ãŒæœ€çµ‚çš„ãªæƒ…å ±ã«ãªã‚Šã¾ã™ã€‚")
                    continue

                job_list = job_list.find_all("div",attrs={"class":"slider_container css-g7s71f eu4oa1w0"})#æ±‚äººã‚’ãã‚Œãã‚Œèª­ã¿è¾¼ã‚€
                print(f"ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººæ•°ã¯{len(job_list)}")


                additional_pay = 0 #è¤‡æ•°ãƒšãƒ¼ã‚¸åˆ†ã®æœ‰æ–™æ±‚äººã¯ã€2å›é€£ç¶šã§0ã ã£ãŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã«æˆ»ã•ã›ã‚‹ãŸã‚åˆ¥å€‹ã‚«ã‚¦ãƒ³ãƒˆ
                for job in job_list:
                    if job.find("span",attrs={"class":"iaIcon"}) and job.find("span",attrs={"class":"more_loc_container"}):#å¿œå‹ŸãŒã‚ã‚Šã€ã‹ã¤ãã®ä»–ã®å‹¤å‹™åœ°ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
                        other_area = job.find("span",attrs={"class":"more_loc_container"}).text #ãã®ä»–ã®å‹¤å‹™åœ°(3)
                        other_area_num = re.findall(r"\d+", other_area) #æ•°å­—éƒ¨åˆ†ã®ã¿ã‚’å–ã‚Šå‡ºã™
                        other_area_num = other_area_num[0]
                        other_area_num = int(other_area_num)
                        more += other_area_num

                    if job.find("span",attrs={"class":"iaIcon"}):#indeedã§å¿œå‹Ÿã€ã‚‚ã—ãã¯ã‚«ãƒ³ã‚¿ãƒ³å¿œå‹Ÿã‚’æ¤œå‡º(ãã®å·¦ã«ã‚ã‚‹çŸ¢å°ã‚¢ã‚¤ã‚³ãƒ³ã‚’åˆ¤å®šã«ä½¿ã£ã¦ã„ã‚‹)
                        additional_pay += 1

                    else:
                        continue

                print(f"ã“ã®ãƒšãƒ¼ã‚¸ã®æœ‰æ–™ã®æ•°ã¯{additional_pay}å€‹")
                print(f"ä»Šã¾ã§ã®æœ‰æ–™ã®æ•°ã¯ç·è¨ˆ{pay + additional_pay}å€‹")
                print(f"ãã®ä»–ã®å‹¤å‹™åœ°ã‚‚å«ã‚ã‚‹ã¨ç·è¨ˆ{pay +additional_pay + more}å€‹ã«ãªã‚Šã¾ã™")


                if len(job_list) is None:#0ã®å ´åˆã€ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚‹
                    print("æ®‹ã‚Šãƒšãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚Šã¾ã™ã€‚")
                    url_count = 0
                    break

                if len(job_list) >= 15:
                    if soup.find("a",attrs={"data-testid":"pagination-page-next"}): #â†’(ä»¥é™ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã¨æ¶ˆãˆã‚‹ï¼‰ãŒã‚ã‚‹ã¨ãƒ«ãƒ¼ãƒ—çµ‚äº†ï¼ˆãƒšãƒ¼ã‚¸ã«ãƒãƒƒã‚¯ã‚¹æ±‚äººã‚ã‚‹ã‘ã©ã€ä»¥é™ã®ãƒšãƒ¼ã‚¸ãŒãªã„æ™‚ã®å¯¾å‡¦)
                        print("ã¾ã ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã«é·ç§»ã—ã¾ã™")
                        continue
                    else:
                        print("ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººæ•°ã¯15ã§ã™ãŒã€ä»¥é™ã®ãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ãªã„ã®ã§ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚Šã¾ã™ã€‚")
                        url_count = 0
                        break

                if len(job_list) < 15:
                    url_count = 0
                    print("æ±‚äººæ•°ãŒ15æœªæº€ã®ãŸã‚ã€ã“ã®ãƒšãƒ¼ã‚¸ã¯æœ€çµ‚ãƒšãƒ¼ã‚¸ã¨ãªã‚Šã¾ã™ã€‚ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚Šã¾ã™ã€‚")
                    break


            if url_count >= 10: #2å›ç›®ä»¥é™(3ãƒšãƒ¼ã‚¸ç›®é·ç§»æ™‚ä»¥é™)
                print("æ±‚äººæ•°ãŒè¦å®šæ•°ä»¥ä¸Šã®ãŸã‚ã€ãƒšãƒ¼ã‚¸ã‚’é·ç§»ã—ã¾ã™")
                url_count+=10
                cur_url = browser.current_url
                cur_url = cur_url.replace(cur_url.split("&")[2],"")
                cur_url_next = cur_url +"&&start=" + str(url_count)
                print(cur_url)
                print(cur_url_next)
                browser.get(f"{cur_url_next}")
                page_num +=1


                sleep_time = random.uniform(8,12) #é·ç§»å‰ã«æ™‚é–“ã‚’ç½®ã
                time.sleep(sleep_time)

                 ##ã“ã“ã‹ã‚‰æ¬¡ã®ãƒšãƒ¼ã‚¸

                source = browser.page_source
                soup = BeautifulSoup(source,"html.parser")

                job_list = soup.find("ul",attrs={"class":"jobsearch-ResultsList css-0"})
                if job_list is None: #è¤‡æ•°ãƒšãƒ¼ã‚¸ç›®ã®æ±‚äººæ•°ãŒ0ã®æ™‚ã¯ã“ã“ã§ä½•ã‚‚ã›ãšãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚‹
                    print("ã“ã®ãƒšãƒ¼ã‚¸ç›®ã«ã¯æ±‚äººãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸğŸ˜¢ç›´ä¸Šã®æƒ…å ±ãŒæœ€çµ‚çš„ãªæƒ…å ±ã«ãªã‚Šã¾ã™ã€‚")
                    continue

                job_list = job_list.find_all("div",attrs={"class":"slider_container css-g7s71f eu4oa1w0"})#æ±‚äººã‚’ãã‚Œãã‚Œèª­ã¿è¾¼ã‚€
                print(f"ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººæ•°ã¯{len(job_list)}")


                additional_pay = 0
                for job in job_list:
                    if job.find("span",attrs={"class":"iaIcon"}) and job.find("span",attrs={"class":"more_loc_container"}):#å¿œå‹ŸãŒã‚ã‚Šã€ã‹ã¤ãã®ä»–ã®å‹¤å‹™åœ°ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
                        other_area = job.find("span",attrs={"class":"more_loc_container"}).text #ãã®ä»–ã®å‹¤å‹™åœ°(3)
                        other_area_num = re.findall(r"\d+", other_area) #æ•°å­—éƒ¨åˆ†ã®ã¿ã‚’å–ã‚Šå‡ºã™
                        other_area_num = other_area_num[0]
                        other_area_num = int(other_area_num)
                        more += other_area_num

                    if job.find("span",attrs={"class":"iaIcon"}):#indeedã§å¿œå‹Ÿã€ã‚‚ã—ãã¯ã‚«ãƒ³ã‚¿ãƒ³å¿œå‹Ÿã‚’æ¤œå‡º(ãã®å·¦ã«ã‚ã‚‹çŸ¢å°ã‚¢ã‚¤ã‚³ãƒ³ã‚’åˆ¤å®šã«ä½¿ã£ã¦ã„ã‚‹)
                        additional_pay += 1

                    else:
                        continue
                # è¿½åŠ æ±‚äººã®åˆ¤å®šã®ãŸã‚ã«ä½¿ç”¨(0ãªã‚‰ï¼‘è¶³ã™ã€ã‚‚ã—ã‚ã£ãŸå ´åˆã¯0ã«ãƒªã‚»ãƒƒãƒˆ)
                if additional_pay == 0:
                    additional_pay_none +=1
                if additional_pay > 0:
                    additional_pay_none = 0

                print(f"ã“ã®ãƒšãƒ¼ã‚¸ã®æœ‰æ–™ã®æ•°ã¯{additional_pay}å€‹")
                print(f"ä»Šã¾ã§ã®æœ‰æ–™ã®æ•°ã¯ç·è¨ˆ{pay + additional_pay}å€‹")
                print(f"ãã®ä»–ã®å‹¤å‹™åœ°ã‚‚å«ã‚ã‚‹ã¨ç·è¨ˆ{pay +additional_pay + more}å€‹ã«ãªã‚Šã¾ã™")


                if additional_pay_none >= 2:
                    print("äºŒå›é€£ç¶šã§æœ‰æ–™æ±‚äººæ•°ãŒ0ã ã£ãŸã®ã§è¤‡æ•°ãƒšãƒ¼ã‚¸é·ç§»ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    url_count = 0
                    break

                if len(job_list) is None:#0ã®å ´åˆã€ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚‹
                    print("æ®‹ã‚Šãƒšãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚Šã¾ã™ã€‚")
                    url_count = 0
                    break

                if len(job_list) >= 15:
                    if soup.find("a",attrs={"data-testid":"pagination-page-next"}): #â†’(ä»¥é™ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã¨æ¶ˆãˆã‚‹ï¼‰ãŒã‚ã‚‹ã¨ãƒ«ãƒ¼ãƒ—çµ‚äº†ï¼ˆãƒšãƒ¼ã‚¸ã«ãƒãƒƒã‚¯ã‚¹æ±‚äººã‚ã‚‹ã‘ã©ã€ä»¥é™ã®ãƒšãƒ¼ã‚¸ãŒãªã„æ™‚ã®å¯¾å‡¦)
                        print("ã¾ã ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã«é·ç§»ã—ã¾ã™")
                        continue
                    else:
                        print("ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººæ•°ã¯15ã§ã™ãŒã€ä»¥é™ã®ãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ãªã„ã®ã§ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚Šã¾ã™ã€‚")
                        url_count =0
                        break

                if len(job_list) < 15:
                    url_count = 0
                    print("æ±‚äººæ•°ãŒ15æœªæº€ã®ãŸã‚ã€ã“ã®ãƒšãƒ¼ã‚¸ã¯æœ€çµ‚ãƒšãƒ¼ã‚¸ã¨ãªã‚Šã¾ã™ã€‚ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚Šã¾ã™ã€‚")
                    break



        if "èª°ã‚ˆã‚Šã‚‚æ—©ã" in soup.text: #é‚ªé­”ãªãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’æ¤œå‡ºã—ãƒªãƒ­ãƒ¼ãƒ‰ã§å‰Šé™¤
            print("é‚ªé­”è€…ã‚’æ¤œå‡º")
            error_count+=1
            print(count)
            print(f"ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆ:{error_count}")
            browser.refresh()
            """  sleep_time = random.uniform(8,12)
            time.sleep(sleep_time)"""

            #æ›´æ–°ã™ã‚‹ã“ã¨ã§æ¶ˆãˆã‚‹ã‚ˆã†ã«ãªã£ãŸ

            """
            whole_page = browser.find_element(By.TAG_NAME,"html")
            print("å…¨ä½“ã‚’å–å¾—")
            browser.maximize_window()
            actions.move_to_element_with_offset(whole_page,1,1)
            actions.click()
            """


"""
    kyujins = soup.find_all("ul",attrs={"class":"jobsearch-ResultsList css-0"})
    kyujins = soup.find_all("li")
    count = 0
    print(len(kyujins))

    for kyujin in kyujins:
        indeed_Apllys = kyujin.find_all("td",attrs={"class":"shelfItem indeedApply"})
        if indeed_Apllys is None:
            print("ã“ã®éƒ¨åˆ†ã®æ±‚äººã«ã¯ã‚ã‚Šã¾ã›ã‚“")
            continue
        else:
            count+=len(indeed_Apllys)

    print(count)
"""






