import random
import time
from selenium import webdriver
from selenium.webdriver.chrome import service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

import requests
from bs4 import BeautifulSoup

import re

import pandas as pd

from selenium.webdriver.common.action_chains import ActionChains

from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome import service as fs

from os.path import join #æ—¢ã«é–‹ã„ã¦ã„ã‚‹ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å–å¾—ã™ã‚‹éš›ã®ã‚³ãƒ¼ãƒ‰ã«å¿…è¦

import chromedriver_binary   #Chromeã®ãƒ‘ã‚¹ã‚’é€šã™ãŸã‚ã®ã‚³ãƒ¼ãƒ‰
#èµ·å‹•ã—ãŸã‚‰ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä½¿ç”¨ã€ãã“ã§é–‹ã‹ã‚ŒãŸãƒ–ãƒ©ã‚¦ã‚¶ã§privacypassã‚’èµ·å‹•ã—èªè¨¼ã‚’è¡Œã†
#/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir="/Users/satousuguru/Desktop/Python/google chrome" âš ï¸æœ€å¾Œã®ã‚¯ã‚ªãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ã‚“å…¥ã‚Œãªã„ã¨ãƒã‚°ã‚‹
#https://qiita.com/mimuro_syunya/items/2464cd2404b67ea5da56:å‚è€ƒ
#:ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä½¿ç”¨

EXTENSION_PATH = "/Users/satousuguru/Desktop/Python/KeywordSurfer.crx" #âš ï¸æ‹¡å¼µæ©Ÿèƒ½ã®ãƒ‘ã‚¹ã€‚é©å®œæ›¸ãæ›ãˆã¦

import csv #csvã‚’èª­ã¿è¾¼ã‚€

browser = webdriver.Chrome()

##æ‰“ã¡è¾¼ã‚€ãƒ¯ãƒ¼ãƒ‰ã€è‡ªç”±ã«æ›¸ãæ›ãˆå¯èƒ½
keyword = "ãƒ‰ãƒ©ã‚¤ãƒãƒ¼"
area_word = "æ‰ä¸¦åŒº"
employment_sta = "æ¥­å‹™å§”è¨—"


global col_names
col_names = ["ä¼šç¤¾å"]
df = pd.DataFrame(columns = col_names)

def set_browser_access():
    CHROMEDRIVER = "/Users/satousuguru/Desktop/chromedriver"
    browser.get("https://jp.indeed.com/")
    source = browser.page_source
    soup = BeautifulSoup(source,"html.parser")


def find_box():
    elem_type_search_word = browser.find_element(By.ID,"text-input-what")
    elem_type_area = browser.find_element(By.ID,"text-input-where")
    return(elem_type_search_word,elem_type_area)


def type_box(keyword,area_word):
    find_box()[0].send_keys(f'"{keyword}"')
    find_box()[1].send_keys(f'"{area_word}"')


def search_btn():
    elem_search_btn = browser.find_element(By.CLASS_NAME,"yosegi-InlineWhatWhere-primaryButton")
    elem_search_btn.click()


def get_soup():
    global soup
    source = browser.page_source
    soup = BeautifulSoup(source,"html.parser")
    return soup


def job_type_btn_find_and_click():
    elem_type_job_button = browser.find_element(By.ID,"filter-jobtype")
    elem_type_job_button.click()


def job_type_change_gyomuitaku():
    cur_url = browser.current_url
    changed_url = cur_url +"&sc=0kf%3Ajt(commission)%3B" #æ¥­å‹™å§”è¨—ã¯ã“ã®æ–‡å­—åˆ—ã‚’URLã«è¶³ã™ã¨é¸æŠã•ã‚Œã‚‹
    browser.get(f"{changed_url}")


def job_type_change_seisyain():
    cur_url = browser.current_url
    changed_url = cur_url +"&sc=0kf%3Ajt(fulltime)%3B&limit=35" #æ­£ç¤¾å“¡ã¯ã“ã®æ–‡å­—åˆ—ã‚’URLã«è¶³ã™ã¨é¸æŠã•ã‚Œã‚‹
    browser.get(f"{changed_url}")


def job_type_change_haken():
    cur_url = browser.current_url
    changed_url = cur_url +"&sc=0kf%3Ajt(temporary)%3B&limit=35" #æ´¾é£ã¯ã“ã®æ–‡å­—åˆ—ã‚’URLã«è¶³ã™ã¨é¸æŠã•ã‚Œã‚‹
    browser.get(f"{changed_url}")

def reload():
    browser.refresh()

def get_companies_and_minus():
    companies = soup.find("ul",attrs={"id":"filter-cmp-menu"})
    if companies is None:
        companies = soup.find("ul",attrs={"id":"filter-fcckey-menu"})
    companies = companies.find_all("li")
    top10_list=[]
    count = 0
    for company in companies:
        company = company.text.split("(")[0]
        top10_list.append(company)
        count += 1
        if count <= 10:
            find_box()[0].send_keys(f'-"{company}"')
    df2 = pd.DataFrame(top10_list ,columns = col_names)
    global df
    df = df.append(df2)
    #ãƒˆãƒƒãƒ—10ç¤¾ã®dfã‚’è¿”ã™
    return df


def get_jobs():
    job_list = soup.find("ul",attrs={"class":"jobsearch-ResultsList css-0"})
    if job_list is None:
        print("ãƒˆãƒƒãƒ—10ç¤¾ã‚’ãƒã‚¤ãƒŠã‚¹æ¤œç´¢ã—ãŸçµæœã€æ±‚äººã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    job_list = job_list.find_all("div",attrs={"class":"slider_container css-g7s71f eu4oa1w0"})#æ±‚äººã‚’ãã‚Œãã‚Œèª­ã¿è¾¼ã‚€
    return job_list

def get_kantan_or_directapply_and_to_df():
    kantan_or_directapply_list = []
    for job in get_jobs():
        if job.find("span",attrs={"class":"iaIcon"}):#indeedã§å¿œå‹Ÿã€ã‚‚ã—ãã¯ã‚«ãƒ³ã‚¿ãƒ³å¿œå‹Ÿã‚’æ¤œå‡º(ãã®å·¦ã«ã‚ã‚‹çŸ¢å°ã‚¢ã‚¤ã‚³ãƒ³ã‚’åˆ¤å®šã«ä½¿ã£ã¦ã„ã‚‹)
            job = job.find("span",attrs={"class":"companyName"})
            kantan_or_directapply_list.append(job.text)

    col_names = ["ä¼šç¤¾å"]
    df2 = pd.DataFrame(kantan_or_directapply_list,columns = col_names)
    global df
    df = df.append(df2)
    #ãƒˆãƒƒãƒ—10ç¤¾ã¨ã€æ¥­å‹™å§”è¨—ã®æ™‚ã«æœ‰æ–™ã‚ã‚‹ç¤¾åã‚’è¿”ã™
    return df


def delete_same():
    global df
    global df_completed
    df_list = df['ä¼šç¤¾å'].to_list()
    re_df = list(set(df_list))
    df_completed = pd.DataFrame(re_df ,columns = col_names)
    print("sameã‚’å‰Šé™¤")
    print(df_completed)


def to_csv():
    global df_completed
    df_completed.to_csv("tokuma2a.csv",index=False)


def sleep():
    time.sleep(10)


def if_plural_page():
    url_count=0
    while len(job_list) == 15: #ãƒšãƒ¼ã‚¸ãŒã¾ã ç¶šãå ´åˆã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã«ç§»å‹•
            if url_count == 0: #æœ€åˆã®å ´åˆ(urlã®æ§‹é€ ãŒé•ã†)
                print("æ±‚äººæ•°ãŒè¦å®šæ•°ä»¥ä¸Šã®ãŸã‚ã€ãƒšãƒ¼ã‚¸ã‚’é·ç§»ã—ã¾ã™")
                url_count+=10
                cur_url = browser.current_url
                cur_url = cur_url.split("&")[0]
                cur_url_next = cur_url +"&&start=" + str(url_count)
                print(cur_url)
                print(cur_url_next)
                browser.get(f"{cur_url_next}")
                page_num +=1

                ##ã“ã“ã‹ã‚‰æ¬¡ã®ãƒšãƒ¼ã‚¸

                source = browser.page_source
                soup = BeautifulSoup(source,"html.parser")

                job_list = soup.find("ul",attrs={"class":"jobsearch-ResultsList css-0"})
                if job_list is None: #2ãƒšãƒ¼ã‚¸ç›®ã®æ±‚äººæ•°ãŒ0ã®æ™‚ã¯ã“ã“ã§ä½•ã‚‚ã›ãšãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚‹
                    print("2ãƒšãƒ¼ã‚¸ç›®ã«ã¯æ±‚äººãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸğŸ˜¢ç›´ä¸Šã®æƒ…å ±ãŒæœ€çµ‚çš„ãªæƒ…å ±ã«ãªã‚Šã¾ã™ã€‚")
                    continue

                job_list = job_list.find_all("div",attrs={"class":"slider_container css-g7s71f eu4oa1w0"})#æ±‚äººã‚’ãã‚Œãã‚Œèª­ã¿è¾¼ã‚€
                print(f"ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººæ•°ã¯{len(job_list)}")



                for job in job_list:
                    if job.find("span",attrs={"class":"iaIcon"}) and job.find("span",attrs={"class":"more_loc_container"}):#å¿œå‹ŸãŒã‚ã‚Šã€ã‹ã¤ãã®ä»–ã®å‹¤å‹™åœ°ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
                        other_area = job.find("span",attrs={"class":"more_loc_container"}).text #ãã®ä»–ã®å‹¤å‹™åœ°(3)
                        other_area_num = re.findall(r"\d+", other_area) #æ•°å­—éƒ¨åˆ†ã®ã¿ã‚’å–ã‚Šå‡ºã™
                        other_area_num = other_area_num[0]
                        other_area_num = int(other_area_num)
                        more += other_area_num

                    if job.find("span",attrs={"class":"iaIcon"}):#indeedã§å¿œå‹Ÿã€ã‚‚ã—ãã¯ã‚«ãƒ³ã‚¿ãƒ³å¿œå‹Ÿã‚’æ¤œå‡º(ãã®å·¦ã«ã‚ã‚‹çŸ¢å°ã‚¢ã‚¤ã‚³ãƒ³ã‚’åˆ¤å®šã«ä½¿ã£ã¦ã„ã‚‹)
                        pay +=1

                    else:
                        continue

                print(f"æœ‰æ–™ã®æ•°ã¯ç·è¨ˆ{pay}å€‹")
                print(f"ãã®ä»–ã®å‹¤å‹™åœ°ã‚‚å«ã‚ã‚‹ã¨ç·è¨ˆ{pay + more}å€‹ã«ãªã‚Šã¾ã™")



                if len(job_list) >= 15:
                    print("ã¾ã ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã«é·ç§»ã—ã¾ã™")
                else:
                    print("æ®‹ã‚Šãƒšãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚Šã¾ã™ã€‚")
                    continue

            if url_count >= 10: #2å›ç›®ä»¥é™(3ãƒšãƒ¼ã‚¸ç›®é·ç§»æ™‚ä»¥é™)
                print("æ±‚äººæ•°ãŒè¦å®šæ•°ä»¥ä¸Šã®ãŸã‚ã€ãƒšãƒ¼ã‚¸ã‚’é·ç§»ã—ã¾ã™")
                url_count+=10
                cur_url = browser.current_url
                cur_url = cur_url.replace(cur_url.split("&")[2],"")
                cur_url_next = cur_url +"&&start=" + str(url_count)
                print(cur_url)
                print(cur_url_next)
                browser.get(f"{cur_url_next}")
                page_num +=1

                 ##ã“ã“ã‹ã‚‰æ¬¡ã®ãƒšãƒ¼ã‚¸

                source = browser.page_source
                soup = BeautifulSoup(source,"html.parser")

                job_list = soup.find("ul",attrs={"class":"jobsearch-ResultsList css-0"})
                if job_list is None: #2ãƒšãƒ¼ã‚¸ç›®ã®æ±‚äººæ•°ãŒ0ã®æ™‚ã¯ã“ã“ã§ä½•ã‚‚ã›ãšãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚‹
                    print("2ãƒšãƒ¼ã‚¸ç›®ã«ã¯æ±‚äººãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸğŸ˜¢ç›´ä¸Šã®æƒ…å ±ãŒæœ€çµ‚çš„ãªæƒ…å ±ã«ãªã‚Šã¾ã™ã€‚")
                    continue

                job_list = job_list.find_all("div",attrs={"class":"slider_container css-g7s71f eu4oa1w0"})#æ±‚äººã‚’ãã‚Œãã‚Œèª­ã¿è¾¼ã‚€
                print(f"ã“ã®ãƒšãƒ¼ã‚¸ã®æ±‚äººæ•°ã¯{len(job_list)}")



                for job in job_list:
                    if job.find("span",attrs={"class":"iaIcon"}) and job.find("span",attrs={"class":"more_loc_container"}):#å¿œå‹ŸãŒã‚ã‚Šã€ã‹ã¤ãã®ä»–ã®å‹¤å‹™åœ°ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
                        other_area = job.find("span",attrs={"class":"more_loc_container"}).text #ãã®ä»–ã®å‹¤å‹™åœ°(3)
                        other_area_num = re.findall(r"\d+", other_area) #æ•°å­—éƒ¨åˆ†ã®ã¿ã‚’å–ã‚Šå‡ºã™
                        other_area_num = other_area_num[0]
                        other_area_num = int(other_area_num)
                        more += other_area_num

                    if job.find("span",attrs={"class":"iaIcon"}):#indeedã§å¿œå‹Ÿã€ã‚‚ã—ãã¯ã‚«ãƒ³ã‚¿ãƒ³å¿œå‹Ÿã‚’æ¤œå‡º(ãã®å·¦ã«ã‚ã‚‹çŸ¢å°ã‚¢ã‚¤ã‚³ãƒ³ã‚’åˆ¤å®šã«ä½¿ã£ã¦ã„ã‚‹)
                        pay +=1

                    else:
                        continue

                print(f"æœ‰æ–™ã®æ•°ã¯{pay}å€‹")
                print(f"ãã®ä»–ã®å‹¤å‹™åœ°ã‚‚å«ã‚ã‚‹ã¨{pay + more}å€‹ã«ãªã‚Šã¾ã™")



                if len(job_list) >= 15:
                    print("ã¾ã ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã«é·ç§»ã—ã¾ã™")
                else:
                    print("æ®‹ã‚Šãƒšãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒ—ã«æˆ»ã‚Šã¾ã™ã€‚")
                    continue

                continue


if __name__ == "__main__": #ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ç›´æ¥èµ·å‹•ã—ã¦ã‚‹æ™‚ã®ã¿ã«å®Ÿè¡Œ
    set_browser_access()
    find_box()
    type_box(keyword,area_word)
    search_btn()
    find_box()
    get_soup()
    get_companies_and_minus()
    search_btn()
    job_type_change_gyomuitaku()
    get_soup()
    get_jobs()
    get_kantan_or_directapply_and_to_df()
    job_type_btn_find_and_click()
    job_type_change_seisyain()
    sleep()
    get_kantan_or_directapply_and_to_df()
    job_type_btn_find_and_click()
    job_type_change_haken()
    sleep()
    get_kantan_or_directapply_and_to_df()
    delete_same()
    to_csv()
